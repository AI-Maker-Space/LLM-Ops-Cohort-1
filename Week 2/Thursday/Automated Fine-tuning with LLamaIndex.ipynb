{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LlamaIndex to Automate the fine-tuning of GPT-3.5-turbo on source documents\n",
    "\n",
    "Primarly Extended from [this](https://colab.research.google.com/drive/1vWeJBXdFEObuihO7Z8ui2CAYkdHQORqo?usp=sharing) notebook, we'll take a look at how we can wrap this process into Chainlit and have our own dynamic fine-tuning machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U llama-index pypdf sentence-transformers ragas openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "openai_api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3322k  100 3322k    0     0  11.6M      0 --:--:-- --:--:-- --:--:-- 11.6M\n"
     ]
    }
   ],
   "source": [
    "!curl https://jaydixit.com/files/PDFs/TheultimateHitchhikersGuide.pdf --output hitchhikers.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"hitchhikers.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "# Shuffle the documents\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(documents)\n",
    "\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    ")\n",
    "\n",
    "question_gen_query = (\n",
    "    \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "    \"a quiz/examination. Using the provided context from a \"\n",
    "    \"report on climate change and the oceans, formulate \"\n",
    "    \"a single question that captures an important fact from the \"\n",
    "    \"context. Restrict the question to the context information provided.\"\n",
    ")\n",
    "\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents[:50],\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_35_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Questions with `gpt-3.5-turbo`\n",
    "\n",
    "We can use the `generate_questions_from_nodes()` method of our dataset generator to produce a number of questions that will be used to fine-tune!\n",
    "\n",
    "> NOTE: This cell will take ~30s-2min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  40  questions\n"
     ]
    }
   ],
   "source": [
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek and see what was created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What did Zaphod find on the external monitor screens in the Horsehead Nebula?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our questions into a text file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Generator\n",
    "\n",
    "Let's generate questions from a different segment of our documents in order to build a robust test for our RAQA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents[\n",
    "        50:\n",
    "    ],  # since we generated ~1 question for 40 documents, we can skip the first 40\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_35_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll use `gpt-3.5-turbo` to generate some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  40  questions\n"
     ]
    }
   ],
   "source": [
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our results for evaluations later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating base `gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load up our evaluation questions and get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is constructing our `VectorIndex` so we can move onto testing the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "# limit the context window to 2048 tokens so that refine is used\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3), context_window=2048\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=gpt_35_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we're actually putting the model to the test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tested our model - let's evaluate it to see how it performed!\n",
    "\n",
    "We're testing our model with the `ragas` framework - found [here](https://github.com/explodinggradients/ragas)\n",
    "\n",
    "You'll notice that we're testing two primary metrics:\n",
    "\n",
    "- [`answer_relevancy`](https://github.com/explodinggradients/ragas/blob/a55c3be8b2389501c5c761df9070126027a4d1d6/src/ragas/metrics/answer_relevance.py#L32): This measures how relevant is the generated answer to the prompt. If the generated answer is incomplete or contains redundant information the score will be low. This is quantified by working out the chance of an LLM generating the given question using the generated answer. Values range (0,1), higher the better.\n",
    "- [`faithfulness`](https://github.com/explodinggradients/ragas/blob/a55c3be8b2389501c5c761df9070126027a4d1d6/src/ragas/metrics/faithfulnes.py#L63): This measures the factual consistency of the generated answer against the given context. This is done using a multi step paradigm that includes creation of statements from the generated answer followed by verifying each of these statements against the context. The answer is scaled to (0,1) range. Higher the better.\n",
    "\n",
    "Read more about their implementations [here](https://github.com/explodinggradients/ragas/blob/main/docs/metrics.md)\n",
    "\n",
    "Again, these cells might take some time to complete - be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/envs/aims-finetune/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:10<00:00, 23.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:52<00:00, 97.41s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.8230, 'answer_relevancy': 0.9308, 'faithfulness': 0.7375}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eval = {'ragas_score': 0.8230, 'answer_relevancy': 0.9308, 'faithfulness': 0.7375}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging `gpt-4` to improve our `gpt-3.5-turbo` base model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-4\", temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"train_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=gpt_4_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this process will take a few minutes. \n",
    "\n",
    "While this is a powerful technique - it is unfortunately quite slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the fine-tuning dataset\n",
    "\n",
    "Now that we have a number of fine-tuning events from our `OpenAIFineTuningHandler()`, let's save them to a `.jsonl` file - the expected format for fine-tuning `gpt-3.5-turbo`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 46 examples to finetuning_events.jsonl\n"
     ]
    }
   ],
   "source": [
    "finetuning_handler.save_finetuning_events(\"finetuning_events.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "file_response = openai.File.create(file=open(\"finetuning_events.jsonl\", \"rb\"), purpose='fine-tune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<File file id=file-6adVuM8Pm7UdksqYKxVklSJg at 0x7f2dd6f1cdd0> JSON: {\n",
       "  \"object\": \"file\",\n",
       "  \"id\": \"file-6adVuM8Pm7UdksqYKxVklSJg\",\n",
       "  \"purpose\": \"fine-tune\",\n",
       "  \"filename\": \"file\",\n",
       "  \"bytes\": 200354,\n",
       "  \"created_at\": 1692905441,\n",
       "  \"status\": \"uploaded\",\n",
       "  \"status_details\": null\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "response = None\n",
    "\n",
    "while not response:\n",
    "  try:\n",
    "    response = openai.FineTuningJob.create(training_file=file_response.id, model=\"gpt-3.5-turbo\")\n",
    "  except:\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-SUD1CJJwBbTvrkV2G5K6bkUY at 0x7f2dd6f1d250> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-SUD1CJJwBbTvrkV2G5K6bkUY\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1692905472,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-VrtlDUDw6aAbBGBDNn1W797P\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"created\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-6adVuM8Pm7UdksqYKxVklSJg\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": null\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_id = response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-SUD1CJJwBbTvrkV2G5K6bkUY at 0x7f2dd710a750> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-SUD1CJJwBbTvrkV2G5K6bkUY\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1692905472,\n",
       "  \"finished_at\": null,\n",
       "  \"fine_tuned_model\": null,\n",
       "  \"organization_id\": \"org-VrtlDUDw6aAbBGBDNn1W797P\",\n",
       "  \"result_files\": [],\n",
       "  \"status\": \"running\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-6adVuM8Pm7UdksqYKxVklSJg\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": null\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTuningJob.retrieve(training_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-ioAEPvMGT9I4dJz2hVw9EXc0\",\n",
      "      \"created_at\": 1692906144,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Fine-tuning job successfully completed\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-UtuXIbtM5Ht7tZTnYFOunmDF\",\n",
      "      \"created_at\": 1692906142,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"New fine-tuned model created: ft:gpt-3.5-turbo-0613:ox::7rAVhh9B\",\n",
      "      \"data\": null,\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-CyQJQN8bxXIOYOVWCKztOqeG\",\n",
      "      \"created_at\": 1692906130,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 130/138: training loss=0.00\",\n",
      "      \"data\": {\n",
      "        \"step\": 130,\n",
      "        \"train_loss\": 0.0009155995794571936,\n",
      "        \"train_mean_token_accuracy\": 1.0\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-9a8bIiuHc8layW4GYZ8DBFox\",\n",
      "      \"created_at\": 1692906118,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 120/138: training loss=0.03\",\n",
      "      \"data\": {\n",
      "        \"step\": 120,\n",
      "        \"train_loss\": 0.02666545659303665,\n",
      "        \"train_mean_token_accuracy\": 1.0\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-b9o1DClrlnuwl4uMrE18p6wP\",\n",
      "      \"created_at\": 1692906106,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 110/138: training loss=0.00\",\n",
      "      \"data\": {\n",
      "        \"step\": 110,\n",
      "        \"train_loss\": 0.0027033654041588306,\n",
      "        \"train_mean_token_accuracy\": 1.0\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-UsHiishNwiyCZnZJm5LFC9yr\",\n",
      "      \"created_at\": 1692906094,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 100/138: training loss=0.47\",\n",
      "      \"data\": {\n",
      "        \"step\": 100,\n",
      "        \"train_loss\": 0.46932321786880493,\n",
      "        \"train_mean_token_accuracy\": 0.9069767594337463\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-i1bj3zcYVTCSuqxONYIkxja1\",\n",
      "      \"created_at\": 1692906081,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 90/138: training loss=0.09\",\n",
      "      \"data\": {\n",
      "        \"step\": 90,\n",
      "        \"train_loss\": 0.08837084472179413,\n",
      "        \"train_mean_token_accuracy\": 0.9375\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-dyKfKkFGEq6eRRA6grEl6PM5\",\n",
      "      \"created_at\": 1692906069,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 80/138: training loss=0.09\",\n",
      "      \"data\": {\n",
      "        \"step\": 80,\n",
      "        \"train_loss\": 0.08918418735265732,\n",
      "        \"train_mean_token_accuracy\": 0.9767441749572754\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-puI76W7mIbnpf2pbIooyhfu6\",\n",
      "      \"created_at\": 1692906059,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 70/138: training loss=0.19\",\n",
      "      \"data\": {\n",
      "        \"step\": 70,\n",
      "        \"train_loss\": 0.18960601091384888,\n",
      "        \"train_mean_token_accuracy\": 0.9333333373069763\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    },\n",
      "    {\n",
      "      \"object\": \"fine_tuning.job.event\",\n",
      "      \"id\": \"ftevent-i5hjEwzdWK1KcWSs4r04P6Yc\",\n",
      "      \"created_at\": 1692906047,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Step 60/138: training loss=0.12\",\n",
      "      \"data\": {\n",
      "        \"step\": 60,\n",
      "        \"train_loss\": 0.12298834323883057,\n",
      "        \"train_mean_token_accuracy\": 0.9285714030265808\n",
      "      },\n",
      "      \"type\": \"metrics\"\n",
      "    }\n",
      "  ],\n",
      "  \"has_more\": true\n",
      "}\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "while openai.FineTuningJob.retrieve(training_id).status == \"running\":\n",
    "  clear_output(wait=True)\n",
    "  time.sleep(5)\n",
    "  print(openai.FineTuningJob.list_events(id=training_id, limit=10))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FineTuningJob fine_tuning.job id=ftjob-SUD1CJJwBbTvrkV2G5K6bkUY at 0x7f2dd72075f0> JSON: {\n",
       "  \"object\": \"fine_tuning.job\",\n",
       "  \"id\": \"ftjob-SUD1CJJwBbTvrkV2G5K6bkUY\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"created_at\": 1692905472,\n",
       "  \"finished_at\": 1692906144,\n",
       "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:ox::7rAVhh9B\",\n",
       "  \"organization_id\": \"org-VrtlDUDw6aAbBGBDNn1W797P\",\n",
       "  \"result_files\": [\n",
       "    \"file-lb9W8pRtbjI5JP4YoZ2AeKtk\"\n",
       "  ],\n",
       "  \"status\": \"succeeded\",\n",
       "  \"validation_file\": null,\n",
       "  \"training_file\": \"file-6adVuM8Pm7UdksqYKxVklSJg\",\n",
       "  \"hyperparameters\": {\n",
       "    \"n_epochs\": 3\n",
       "  },\n",
       "  \"trained_tokens\": 159942\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.FineTuningJob.retrieve(training_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_id = openai.FineTuningJob.retrieve(training_id).fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the fine-tuned model\n",
    "\n",
    "Now that we've fine-tuned our model on the `gpt-4` enhanced question answers - let's see how it performs on our `raga` evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "\n",
    "ft_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=ft_model_id, temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=ft_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:03<00:00, 21.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:22<00:00, 87.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ragas_score': 0.8599, 'answer_relevancy': 0.9398, 'faithfulness': 0.7925}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [answer_relevancy, faithfulness])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_eval = {'ragas_score': 0.8599, 'answer_relevancy': 0.9398, 'faithfulness': 0.7925}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Differences\n",
    "\n",
    "Now we can compare the outputs of the two models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did Arthur see when he moved back and looked again at the solid wall of blue ice?\n"
     ]
    }
   ],
   "source": [
    "print(questions[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Arthur saw the sky when he moved back and looked again at the solid wall of blue ice."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(service_context=gpt_35_context)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "\n",
    "ft_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=ft_model_id, temperature=0.3),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Arthur saw something within the frozen depths."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(service_context=ft_context)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model answer_relevancy : 0.9308\n",
      "Fine-tuned model answer_relevancy : 0.9398\n",
      "Improvement answer_relevancy : 0.90%\n",
      "\n",
      "Base model faithfulness : 0.7375\n",
      "Fine-tuned model faithfulness : 0.7925\n",
      "Improvement faithfulness : 5.50%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric_list = [\"answer_relevancy\", \"faithfulness\"]\n",
    "\n",
    "for metric in metric_list:\n",
    "  print(\"Base model\", metric, \":\", base_eval[metric])\n",
    "  print(\"Fine-tuned model\", metric, \":\", ft_eval[metric])\n",
    "  print(f\"Improvement {metric} : {(ft_eval[metric] - base_eval[metric])*100:.2f}%\")\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims-barbenheimer-chainlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
