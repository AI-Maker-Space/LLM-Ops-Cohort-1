{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith\n",
    "\n",
    "We'll be moving through this notebook to explain what visibility tools can do to help us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q langchain openai langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Application\n",
    "\n",
    "We'll be leveraging the same application as we did in the Weights and Biases Notebook to showcase what can be done with LangSmith:\n",
    "\n",
    "A simple Arxiv Agent!\n",
    "\n",
    "Let's start with grabbing a few pieces of information and our additional dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up our LangChain environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "unique_id = uuid4().hex[0:8]\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LLM Ops Walk Through - {unique_id}\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up our simple application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "tools = load_tools(\n",
    "    [\"arxiv\"]\n",
    ")\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQLoRA might be a term or concept from a scientific field. I should search for it on arxiv to find relevant articles.\n",
      "Action: arxiv\n",
      "Action Input: QLoRA\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2023-05-23\n",
      "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
      "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
      "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
      "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
      "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
      "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
      "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
      "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
      "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
      "single GPU. QLoRA introduces a number of innovations to save memory without\n",
      "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
      "information theoretically optimal for normally distributed weights (b) double\n",
      "quantization to reduce the average memory footprint by quantizing the\n",
      "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
      "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
      "instruction following and chatbot performance across 8 instruction datasets,\n",
      "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
      "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
      "show that QLoRA finetuning on a small high-quality dataset leads to\n",
      "state-of-the-art results, even when using smaller models than the previous\n",
      "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
      "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
      "alternative to human evaluation. Furthermore, we find that current chatbot\n",
      "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
      "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
      "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
      "4-bit training.\n",
      "\n",
      "Published: 2023-07-20\n",
      "Title: IvyGPT: InteractiVe Chinese pathwaY language model in medical domain\n",
      "Authors: Rongsheng Wang, Yaofei Duan, ChanTong Lam, Jiexi Chen, Jiangsheng Xu, Haoming Chen, Xiaohong Liu, Patrick Cheong-Iao Pang, Tao Tan\n",
      "Summary: General large language models (LLMs) such as ChatGPT have shown remarkable\n",
      "success. However, such LLMs have not been widely adopted for medical purposes,\n",
      "due to poor accuracy and inability to provide medical advice. We propose\n",
      "IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality\n",
      "medical question-answer (QA) instances and Reinforcement Learning from Human\n",
      "Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn\n",
      "conversation capabilities, but it cannot perform like a doctor in other\n",
      "aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output\n",
      "richer diagnosis and treatment answers that are closer to human. In the\n",
      "training, we used QLoRA to train 33 billion parameters on a small number of\n",
      "NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed\n",
      "other medical GPT models.\n",
      "\n",
      "Published: 2023-08-20\n",
      "Title: LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models\n",
      "Authors: Yixuan Weng, Zhiqi Wang, Huanxuan Liao, Shizhu He, Shengping Liu, Kang Liu, Jun Zhao\n",
      "Summary: With the burgeoning development in the realm of large language models (LLMs),\n",
      "the demand for efficient incremental training tailored to specific industries\n",
      "and domains continues to increase. Currently, the predominantly employed\n",
      "frameworks lack modular design, it often takes a lot of coding work to\n",
      "kickstart the training of LLM. To address this, we present \"LMTuner\", a highly\n",
      "usable, integrable, and scalable system for training LLMs expeditiously and\n",
      "with minimal user-input. LMTuner comprises three main modules - the\n",
      "Interaction, Training, and Inference Modules. We advocate that LMTuner's\n",
      "usability and integrality alleviate the complexi\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQLoRA is an efficient finetuning approach that reduces memory usage enough to finetune a large parameter model on a single GPU while preserving full 16-bit finetuning task performance. It backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). QLoRA introduces a number of innovations to save memory without sacrificing performance, including a new data type that is information theoretically optimal for normally distributed weights, double quantization to reduce the average memory footprint by quantizing the quantization constants, and paged optimizers to manage memory spikes. It has been used to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across multiple model types and scales.\n",
      "Final Answer: QLoRA is an efficient finetuning approach for large language models that reduces memory usage while preserving performance. It introduces several innovations including a new data type for normally distributed weights, double quantization, and paged optimizers. It has been used to finetune a wide range of models, providing detailed analysis of their performance.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is QLoRA?',\n",
       " 'output': 'QLoRA is an efficient finetuning approach for large language models that reduces memory usage while preserving performance. It introduces several innovations including a new data type for normally distributed weights, double quantization, and paged optimizers. It has been used to finetune a wide range of models, providing detailed analysis of their performance.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain(\"What is QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a number of input/output pairs that we can leverage later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "inputs = [\n",
    "    \"What optimizer is used in QLoRA?\",\n",
    "    \"What data type was created in the QLoRA paper?\",\n",
    "    \"What is a Retrieval Augmented Generation system?\",\n",
    "    \"Who authored the QLoRA paper?\",\n",
    "    \"What is the most popular deep learning framework?\",\n",
    "    \"What significant improvements does the LoRA system make?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "async def arun(agent, input_example):\n",
    "    try:\n",
    "        return await agent.arun(input_example)\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "for input_example in inputs:\n",
    "    results.append(arun(agent_chain, input_example))\n",
    "\n",
    "results = await asyncio.gather(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've run through all of those chains - we can leverage LangSmith to create a dataset that we can use to benchmark other application solutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.tracers.langchain import wait_for_all_tracers\n",
    "\n",
    "wait_for_all_tracers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating with LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'll need to do is collect our responses into a dataset that we can use to benchmark other solutions against!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = f\"arxiv-rag-gpt-4-{unique_id}\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name, description=\"A dataset for benchmarking a RAG system using the Arxiv tool\"\n",
    ")\n",
    "\n",
    "runs = client.list_runs(\n",
    "    project_name=os.environ[\"LANGCHAIN_PROJECT\"],\n",
    "    execution_order=1,  # Only return the top-level runs\n",
    "    error=False,  # Only runs that succeed\n",
    ")\n",
    "for run in runs:\n",
    "    client.create_example(inputs=run.inputs, outputs=run.outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset set up in LangSmith - let's create another system that we can benchmark against our original!\n",
    "\n",
    "Since it's possible to build an agent that has memory (which could influence results and might not provide accurate benchmarking) - we'll use an `agent_factory` to create our agent for each test-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "tools = load_tools(\n",
    "    [\"arxiv\"]\n",
    ")\n",
    "\n",
    "def agent_factory():\n",
    "    agent_chain = initialize_agent(\n",
    "        tools,\n",
    "        llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "    )\n",
    "    return agent_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `langchain.evaluation.EvaluatorType` and `langchain.smith.RunEvalConfig` methods to build a pipeline for our evaluation.\n",
    "\n",
    "More information about these metrics is found [here](https://docs.smith.langchain.com/evaluation/evaluator-implementations)\n",
    "Let's set it up with the following evluators:\n",
    "\n",
    "- `EvaluatorType.QA` - measures how \"correct\" your response is, based on a reference answer (we built these in the first part of the notebook)\n",
    "- `EvaluatorType.EMBEDDING_DISTANCE` - measure closeness between the two responses\n",
    "- `RunEvalConfig.LabeledCriteria` - measures the output against the given criteria\n",
    "- `RunEvalConfig.Criteria({\"YOUR CUSTOM CRITERAI\", \"DESCRIPTION OF YOUR CRITERIA IN NATURAL LANGUAGE\"})`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also build our own custom evaluator as a demonstration of how to implement such an evaluator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our own custom evaluator we need to make sure of a couple things:\n",
    "\n",
    "1. We provide a system by which we can measure or provide a measure of closeness/some numeric metric.\n",
    "2. We provide logic for implementing our score and parsing the relevant outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Optional\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import StringEvaluator\n",
    "\n",
    "\n",
    "class DopenessEvaluator(StringEvaluator):\n",
    "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "        template = \"\"\"On a scale from 0 to 100, how dope is the following response to the input:\n",
    "        --------\n",
    "        INPUT: {input}\n",
    "        --------\n",
    "        OUTPUT: {prediction}\n",
    "        --------\n",
    "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
    "\n",
    "        self.eval_chain = LLMChain.from_string(llm=llm, template=template)\n",
    "\n",
    "    @property\n",
    "    def requires_input(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def requires_reference(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def evaluation_name(self) -> str:\n",
    "        return \"dopeness_score\"\n",
    "\n",
    "    def _evaluate_strings(\n",
    "        self,\n",
    "        prediction: str,\n",
    "        input: Optional[str] = None,\n",
    "        reference: Optional[str] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> dict:\n",
    "        evaluator_result = self.eval_chain(\n",
    "            dict(input=input, prediction=prediction), **kwargs\n",
    "        )\n",
    "        reasoning, score = evaluator_result[\"text\"].split(\"\\n\", maxsplit=1)\n",
    "        score = re.search(r\"\\d+\", score).group(0)\n",
    "        if score is not None:\n",
    "            score = float(score.strip()) / 100.0\n",
    "        return {\"score\": score, \"dopeness\": reasoning.strip()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set our `RunEvalFeedback` up!\n",
    "\n",
    "Notice how we can create custom evaluations that are string based only - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.smith import RunEvalConfig\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators = [\n",
    "        EvaluatorType.QA,\n",
    "        EvaluatorType.EMBEDDING_DISTANCE,\n",
    "        RunEvalConfig.LabeledCriteria(\"relevance\"),\n",
    "        RunEvalConfig.Criteria({\n",
    "            \"fully_answered\" : \"Does this response fully answer the question?\"\n",
    "        })\n",
    "    ],\n",
    "    custom_evaluators = [\n",
    "        DopenessEvaluator()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import (\n",
    "    arun_on_dataset,\n",
    ")\n",
    "\n",
    "tag_name = f\"<< YOUR NAME HERE>>\"\n",
    "tag = \"GPT-4 vs. GPT-3.5-turbo on Arxiv RAG \" + tag_name\n",
    "\n",
    "chain_results = await arun_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=agent_factory,\n",
    "    evaluation=evaluation_config,\n",
    "    verbose=True,\n",
    "    tags=[tag],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims-visibility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
