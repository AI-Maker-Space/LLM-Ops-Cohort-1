{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Ops Visibility and Caching Strategies\n",
    "\n",
    "# 🏗️ Build\n",
    "\n",
    "You will build an application that leverages a visibility tool (Weights and Biases Promopts) and prompt caching.\n",
    "\n",
    "# 🚢 Ship\n",
    "\n",
    "You will ship that application to a Hugging Face space.\n",
    "\n",
    "# 🚀 Share\n",
    "\n",
    "Create a social media post explaning or showcasing the power of prompt-caching, and visibility tooling in your LLM Ops stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visibility Tools\n",
    "\n",
    "A key part of LLM Ops is having a visibility platform where you can track, trace, and collect, various prompt and user data. \n",
    "\n",
    "Let's take a look at it in this notebook!\n",
    "\n",
    "As always, we'll want to start with our dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U \"wandb>=0.15.4\" \"langchain>=0.0.218\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting started with Weights and Biases Prompts can be as easy as setting the `LANGCHAIN_WANDB_TRACING` environment variable to `true`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"langchain-testing\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"./wandb_notebook.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = getpass.getpass(\"Enter your WandB API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up our simple application!\n",
    "\n",
    "We're going to create an agent with the following characteristics:\n",
    "\n",
    "1. `ChatOpenAI` : `gpt-3.5-turbo` powered, `temperature` set to reduce creativity\n",
    "2. `arxiv` tool\n",
    "3. `ZERO_SHOT_REACT_DESCRIPTION` agent\n",
    "\n",
    "Please refer to the following documentation if you get stuck:\n",
    "\n",
    "- [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\n",
    "- [load_tools](https://api.python.langchain.com/en/latest/agents/langchain.agents.load_tools.load_tools.html)\n",
    "- [initialize_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.initialize.initialize_agent.html)\n",
    "- [AgentType](https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    ### YOUR CODE HERE\n",
    ")\n",
    "\n",
    "tools = load_tools(\n",
    "    ### YOUR CODE HERE\n",
    ")\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    ### YOUR CODE HERE\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain.run(\"What is QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have some outputs - let's see what Weights and Biases was able to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring WandB Outputs\n",
    "\n",
    "First things first, we'll want to head to our WandB home page and find our projects!\n",
    "\n",
    "You'll navigate to `wandb.ai/{YOUR_USERNAME_HERE}` - and then click the `Projects` tab.\n",
    "\n",
    "![image](https://i.imgur.com/mplxa4p.png)\n",
    "\n",
    "Now we can head into our project, which should be named `langchain-testing`:\n",
    "\n",
    "![image](https://i.imgur.com/Q4AU0NC.png)\n",
    "\n",
    "Explore all the tools made available to you through the Prompt Workspace!\n",
    "\n",
    "Let's try another prompt and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain.run(\"What is LLM Ops?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, repeated calls will continue to add more information to our `langchain-testing` project!\n",
    "\n",
    "![image](https://i.imgur.com/Xze6jNE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a slightly more complex application by adding a Prompt Cache!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding A Prompt Cache\n",
    "\n",
    "The basic idea of Prompt Caching is to provide a way to circumvent going to the LLM for prompts we have already seen.\n",
    "\n",
    "Similar to cached embeddings, the idea is simple:\n",
    "\n",
    "- Keep track of all the input/output pairs\n",
    "- If a user query is (in the case of semantic similarity caches) close enough to a previous prompt contained in the cache, return the output associated with that pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a Prompt Cache\n",
    "\n",
    "There are many different tools you can use to implement a Prompt Cache - from a \"build it yourself\" VectorStore implementation - to Redis - to custom libraries - there are upsides and downsides to each solution. \n",
    "\n",
    "Let's look at the Redis-backed Cache vs. `InMemoryCache` as an example:\n",
    "\n",
    "Redis Cache\n",
    "\n",
    "| Pros  | Cons  |\n",
    "|---|---|\n",
    "| Managed and Robust  | Expensive to Host  |\n",
    "| Integrations on all Major Cloud Platforms  | Non-trivial to Integrate |\n",
    "| Easily Scalable  | Does not have a ChatModel implementation |\n",
    "\n",
    "`InMemoryCache`\n",
    "\n",
    "| Pros  | Cons  |\n",
    "|---|---|\n",
    "| Easily implemented  | Consumes potentially precious memory |\n",
    "| Completely Cloud Agnostic  | Does not offer inter-session caching |\n",
    "\n",
    "For the sake of ease of use - and to allow functionality with our `ChatOpenAI` model - we'll leverage `InMemoryCache`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set our `langchain.llm_cache` to use the `InMemoryCache`.\n",
    "\n",
    "- [`InMemoryCache`](https://api.python.langchain.com/en/latest/cache/langchain.cache.InMemoryCache.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more important fact about the `InMemoryCache` is that it is what's called an \"exact-match\" cache - meaning it will only trigger when the user query is *exactly* represented in the cache. \n",
    "\n",
    "This is a safer cache, as we can guarentee the user's query exactly matches with previous queries and we don't have to worry about edge-cases where semantic similarity might fail - but it does reduce the potential to hit the cache.\n",
    "\n",
    "We could leverage tools like `GPTCache`, or `RedisCache` (for non-chat model implementations) to get a \"semantic similarity\" cache, if desired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "agent_chain.run(\"What is Retrieval Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's great! Working as expected - let's take a look at the output of our `ChatOpenAI` module in our Weights and Biases project:\n",
    "\n",
    "```\n",
    "{ \"token_usage\": { \"prompt_tokens\": 1057, \"completion_tokens\": 130, \"total_tokens\": 1187 }, \"model_name\": \"gpt-3.5-turbo-0613\" }\n",
    "```\n",
    "\n",
    "So, you can see: We used `1187` total tokens, and the request took ~8s.\n",
    "\n",
    "Let's look at the full output of our Weights and Biases project:\n",
    "\n",
    "![image](https://i.imgur.com/cU8NuDK.png)\n",
    "\n",
    "Let's try the same request again and see what happens this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "agent_chain.run(\"What is Retrieval Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right away, we can see that the chain only took ~0.3s, very promising! Let's check in WandB!\n",
    "\n",
    "This time, we cannot find information about token usage in Weights and Biases because we never actually needed to hit OpenAI's endpoint. \n",
    "\n",
    "Let's look at the Weights and Biases project output:\n",
    "\n",
    "![image](https://i.imgur.com/UjPsC6x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see - we completely bypass the chain - and directly return the previous result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Your task is to include both a prompt cache, and visibility to your application in any of your previous assignments, wrap it up in a Chainlit application, and host it on a Hugging Face Space (or EC2)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims-visibility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
